# -*- coding: utf-8 -*-
"""movie_review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m-HS4QcPyxxZ7sG6reEP6xynSX0sHbOf
"""

from google.colab import drive

drive.mount('/content/drive')

from google.colab import drive

drive.mount('/content/drive')

import pandas as pd

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from sklearn.preprocessing import LabelEncoder

import warnings
warnings.filterwarnings('ignore')
sns.set()

imdb = pd.read_csv('/content/drive/MyDrive/data/IMDB Dataset.csv')

imdb.head()

imdb.sentiment.value_counts()

text = imdb['review'][0]
print(text)
print("<==========>")
print(word_tokenize(text))

corpus = []
for text in imdb['review']:
  words = [word.lower() for word in word_tokenize(text)]
  corpus.append(words)

num_words = len(corpus)
print(num_words)

imdb.shape

train_size = int(imdb.shape[0]*0.8)
x_train = imdb.review[:train_size]
y_train = imdb.sentiment[:train_size]

x_test = imdb.review[train_size:]
y_test = imdb.sentiment[train_size:]

tokenizer = Tokenizer(num_words)
tokenizer.fit_on_texts(x_train)
x_train = tokenizer.texts_to_sequences(x_train)
x_train = pad_sequences(x_train, maxlen=128, truncating='post', padding='post')

x_train[0], len(x_train[0])

x_test = tokenizer.texts_to_sequences(x_test)
x_test = pad_sequences(x_test, maxlen=128, truncating='post', padding='post')

x_test[0], len(x_test[0])

print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.transform(y_test)

model = Sequential()

model.add(Embedding(input_dim=num_words, output_dim=100, input_length=128, trainable=True))
model.add(LSTM(100, dropout=0.1, return_sequences=True))
model.add(LSTM(100, dropout=0.1))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

history = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=[x_test, y_test])

plt.figure(figsize=(16,5))
epochs = range(1, len(history.history['accuracy'])+1)
plt.plot(epochs, history.history['loss'], 'b', label='Training Loss', color='red')
plt.plot(epochs, history.history['val_loss'], 'b', label='Validate Loss', color='green')
plt.legend()
plt.show()

plt.figure(figsize=(16,5))
epochs = range(1, len(history.history['accuracy'])+1)
plt.plot(epochs, history.history['accuracy'], 'b', label='Training Accuracy', color='red')
plt.plot(epochs, history.history['val_accuracy'], 'b', label='Validation Accuracy', color='green')
plt.legend()
plt.show()

validation_sentence = ['This movie is wonderful, I Love it.']
validation_sentence_tokened = tokenizer.texts_to_sequences(validation_sentence)
validation_sentence_padded = pad_sequences(validation_sentence_tokened, maxlen=128, truncating='post', padding='post')

print(validation_sentence[0])
print("Probability of Positive: {}" .format(model.predict(validation_sentence_padded)[0]))

validation_sentence = ['Waste of time and money. Not at all a good movie.']
validation_sentence_tokened = tokenizer.texts_to_sequences(validation_sentence)
validation_sentence_padded = pad_sequences(validation_sentence_tokened, maxlen=128, truncating='post', padding='post')

print(validation_sentence[0])
print("Probability of Positive: {}" .format(model.predict(validation_sentence_padded)[0]))

